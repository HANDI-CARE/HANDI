"""
Drug LLM Test Router ì „ìš© LangSmith íŠ¸ë ˆì´ì‹± ì„œë¹„ìŠ¤
ê¸°ì¡´ drug_analysis_serviceì˜ ë©”ì„œë“œë“¤ì„ ë³µì‚¬í•˜ë˜ LangSmith íŠ¸ë ˆì´ì‹±ë§Œ ì¶”ê°€
"""
import time
import json
import random
from typing import Dict, Any
from langsmith import traceable
from app.core.config.config import settings
from app.core.logger import logger
from app.services.drug_analysis_service import drug_analysis_service
from app.services.langsmith_config import langsmith_manager
from app.services.evaluation_service import evaluation_service
from app.schemas.drug_analysis import (
    DrugInfoAnalysisRequest, PerformanceComparisonRequest, 
    PerformanceComparisonResponse, PerformanceComparisonNRequest, 
    PerformanceComparisonNResponse
)

class DrugLLMTestService:
    """Drug LLM Test Router ì „ìš© ì„œë¹„ìŠ¤ - LangSmith íŠ¸ë ˆì´ì‹± í¬í•¨"""
    
    def __init__(self):
        # ê¸°ì¡´ drug_analysis_serviceë¥¼ ë˜í•‘
        self.base_service = drug_analysis_service
        # ëœë¤ ë…¸ì¸ í™˜ì ë…¸íŠ¸ ìƒ˜í”Œ ë°ì´í„°
        self.elderly_notes = [
            "ê·œì¹™ì ì¸ ì‚°ì±…ì„ ì¦ê¸°ì‹¬",
            "í˜ˆì••ì•½ì„ ì¥ê¸° ë³µìš© ì¤‘ì´ì‹œë©° ì–´ì§€ëŸ¬ì›€ì„ ìì£¼ í˜¸ì†Œí•˜ì‹¬",
            "ë‹¹ë‡¨ê°€ ìˆì–´ í˜ˆë‹¹ ê´€ë¦¬ê°€ í•„ìš”í•˜ì‹œë©° ë‹¨ ìŒì‹ì„ í”¼í•˜ë ¤ ë…¸ë ¥í•˜ì‹¬", 
            "ê´€ì ˆì—¼ìœ¼ë¡œ ë¬´ë¦ í†µì¦ì´ ì‹¬í•˜ì—¬ ê³„ë‹¨ ì´ìš©ì´ ì–´ë ¤ìš°ì‹¬",
            "ì¹˜ë§¤ ì´ˆê¸° ì¦ìƒìœ¼ë¡œ ê¸°ì–µë ¥ ì €í•˜ì™€ í˜¼ë™ì´ ì¢…ì¢… ìˆìœ¼ì‹¬",
            "ìœ„ì¥ì´ ì•½í•˜ì—¬ ë§¤ìš´ ìŒì‹ê³¼ ì°¬ ìŒì‹ì„ í”¼í•˜ì‹œê³  ì†Œí™”ì œë¥¼ ìì£¼ ë³µìš©í•˜ì‹¬",
            "ë¶ˆë©´ì¦ìœ¼ë¡œ ë°¤ì— ì ë“¤ê¸° ì–´ë ¤ì›Œí•˜ì‹œë©° ìˆ˜ë©´ì œ ë³µìš© ê²½í—˜ì´ ìˆìœ¼ì‹¬",
            "ì‹¬ì¥ì§ˆí™˜ìœ¼ë¡œ ê²©í•œ ìš´ë™ì„ í”¼í•˜ì‹œê³  ì •ê¸° ê²€ì§„ì„ ë°›ê³  ê³„ì‹¬",
            "ë‚™ìƒ ê²½í—˜ì´ ìˆì–´ ë³´í–‰ ì‹œ ì¡°ì‹¬ìŠ¤ëŸ¬ìš°ì‹œë©° ì§€íŒ¡ì´ë¥¼ ì‚¬ìš©í•˜ì‹¬",
            "ìš°ìš¸ê°ì„ ìì£¼ í˜¸ì†Œí•˜ì‹œë©° ê°€ì¡±ê³¼ì˜ ëŒ€í™”ë¥¼ ì„ í˜¸í•˜ì‹¬"
        ]
    
    @traceable(
        name="drug_analysis_stuff_llm_processing",
        tags=["abtest", "A"],
        metadata={
            "method": "A",
            "model": settings.LLM_MODEL_NAME,
            "temperature": str(settings.LLM_TEMPERATURE),
            "approach": "stuff_documents",
            "processing": "sequential"
        }
    )
    async def analyze_drug_info_stuff_traced(self, request: DrugInfoAnalysisRequest):
        """Stuff ë°©ì‹ ì•½ë¬¼ ë¶„ì„ (LangSmith íŠ¸ë ˆì´ì‹± í¬í•¨)"""
        return await self.base_service.analyze_drug_info(request)
    
    @traceable(
        name="drug_analysis_langchain_llm_processing", 
        tags=["abtest", "B"],
        metadata={
            "method": "B",
            "model": settings.LLM_MODEL_NAME,
            "temperature": str(settings.LLM_TEMPERATURE),
            "approach": "map_reduce_chain",
            "processing": "parallel"
        }
    )
    async def analyze_drug_info_langchain_traced(self, request: DrugInfoAnalysisRequest):
        """LangChain ë°©ì‹ ì•½ë¬¼ ë¶„ì„ (LangSmith íŠ¸ë ˆì´ì‹± í¬í•¨)"""
        return await self.base_service.analyze_drug_info_langchain(request)
    
    @traceable(
        name="drug_analysis_performance_comparison",
        tags=["abtest", "comparison"],
        metadata={
            "method": "comparison",
            "model": settings.LLM_MODEL_NAME,
            "temperature": str(settings.LLM_TEMPERATURE),
            "approach": "performance_analysis",
            "comparison_type": "A_vs_B"
        }
    )
    async def compare_performance_traced(self, request: PerformanceComparisonRequest) -> PerformanceComparisonResponse:
        """ì„±ëŠ¥ ë¹„êµ (LangSmith íŠ¸ë ˆì´ì‹± í¬í•¨)"""
        # ê¸°ì¡´ ì„œë¹„ìŠ¤ì˜ compare_performance ë¡œì§ì„ ë³µì‚¬í•˜ë˜ íŠ¸ë ˆì´ì‹±ëœ ë©”ì„œë“œ ì‚¬ìš©
        try:
            total_start_time = time.time()
            
            # DrugInfoAnalysisRequestë¡œ ë³€í™˜ (note ì •ë³´ í¬í•¨)
            current_note = getattr(self, '_current_note', None)
            analysis_request = DrugInfoAnalysisRequest(drug_summary=request.drug_summary, note=current_note)
            
            from app.schemas.drug_analysis import MethodResult
            stuff_result = MethodResult(success=False)
            langchain_result = MethodResult(success=False)
            
            # 1. Stuff ë°©ì‹ ì‹¤í–‰ (íŠ¸ë ˆì´ì‹± í¬í•¨)
            try:
                stuff_response = await self.analyze_drug_info_stuff_traced(analysis_request)
                stuff_result = MethodResult(
                    success=True,
                    result=stuff_response.analysis_result,
                    data_collection_time=stuff_response.data_collection_time,
                    processing_time=stuff_response.processing_time,
                    error=None
                )
            except Exception as e:
                stuff_result = MethodResult(
                    success=False,
                    result=None,
                    data_collection_time=0.0,
                    processing_time=0.0,
                    error=str(e)
                )
            
            # 2. LangChain ë°©ì‹ ì‹¤í–‰ (íŠ¸ë ˆì´ì‹± í¬í•¨)
            try:
                langchain_response = await self.analyze_drug_info_langchain_traced(analysis_request)
                langchain_result = MethodResult(
                    success=True,
                    result=langchain_response.analysis_result,
                    data_collection_time=langchain_response.data_collection_time,
                    processing_time=langchain_response.processing_time,
                    error=None
                )
            except Exception as e:
                langchain_result = MethodResult(
                    success=False,
                    result=None,
                    data_collection_time=0.0,
                    processing_time=0.0,
                    error=str(e)
                )
            
            # 3. ì„±ëŠ¥ ë¹„êµ ë¶„ì„
            comparison = None
            if stuff_result.success and langchain_result.success:
                stuff_llm_time = stuff_result.processing_time
                langchain_llm_time = langchain_result.processing_time
                
                if stuff_llm_time < langchain_llm_time:
                    faster_method = "Stuff"
                    performance_improvement = ((langchain_llm_time - stuff_llm_time) / langchain_llm_time) * 100
                else:
                    faster_method = "LangChain"  
                    performance_improvement = ((stuff_llm_time - langchain_llm_time) / stuff_llm_time) * 100
                
                comparison = {
                    "stuff_llm_time": round(stuff_llm_time, 2),
                    "langchain_llm_time": round(langchain_llm_time, 2),
                    "time_difference": round(abs(stuff_llm_time - langchain_llm_time), 2),
                    "faster_method": faster_method,
                    "performance_improvement": round(performance_improvement, 1),
                    "stuff_data_collection": stuff_result.data_collection_time,
                    "langchain_data_collection": langchain_result.data_collection_time
                }
                
            
            total_processing_time = round(time.time() - total_start_time, 2)
            
            return PerformanceComparisonResponse(
                stuff_result=stuff_result,
                langchain_result=langchain_result,
                comparison=comparison,
                total_processing_time=total_processing_time,
                drug_count=len(request.drug_summary)
            )
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë¹„êµ ì¤‘ ì „ì²´ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise e
    
    async def compare_performance_n_times_traced(self, request: PerformanceComparisonNRequest) -> PerformanceComparisonNResponse:
        """NíšŒ ë°˜ë³µ ì„±ëŠ¥ ë¹„êµ (LangSmith íŠ¸ë ˆì´ì‹± í¬í•¨)"""
        # NíšŒ ë°˜ë³µì—ì„œëŠ” ê° roundë§ˆë‹¤ íŠ¸ë ˆì´ì‹±ëœ ë©”ì„œë“œë“¤ì„ ì‚¬ìš©í•´ì•¼ í•˜ë¯€ë¡œ
        # ê¸°ì¡´ ì„œë¹„ìŠ¤ì˜ ë¡œì§ì„ ë³µì‚¬í•˜ë˜ íŠ¸ë ˆì´ì‹±ëœ ë©”ì„œë“œ í˜¸ì¶œ
        try:
            total_start_time = time.time()
            test_rounds = []
            
            logger.info(f"ğŸ² [ëœë¤ ì„ íƒ ëª¨ë“œ] NíšŒ í…ŒìŠ¤íŠ¸ ì‹œì‘ - {request.test_count}íšŒ, ë§¤ë²ˆ 2ê°œì”© ëœë¤ ì„ íƒ")
            
            for round_num in range(1, request.test_count + 1):
                
                # ChromaDBì—ì„œ ëœë¤ìœ¼ë¡œ 2ê°œ ì•½ë¬¼ ì„ íƒ
                random_drugs = await self.base_service._get_random_drugs(2)
                from app.schemas.drug_analysis import DrugSummaryItem
                drug_items = [DrugSummaryItem(name=drug['name'], capacity=drug['capacity']) for drug in random_drugs]
                
                # ëœë¤ ë…¸ì¸ í™˜ì ë…¸íŠ¸ ì„ íƒ (50% í™•ë¥ ë¡œ ì¶”ê°€)
                random_note = random.choice(self.elderly_notes) if random.random() < 0.5 else None
                
                # ì„±ëŠ¥ ë¹„êµ ìš”ì²­ ìƒì„± (noteëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬)
                from app.schemas.drug_analysis import PerformanceComparisonRequest
                test_request = PerformanceComparisonRequest(drug_summary=drug_items)
                
                # note ì •ë³´ë¥¼ ì„ì‹œë¡œ ì €ì¥í•˜ì—¬ ì„±ëŠ¥ ë¹„êµ ì‹œ ì‚¬ìš©
                self._current_note = random_note
                
                # íŠ¸ë ˆì´ì‹±ëœ ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰
                comparison_result = await self.compare_performance_traced(test_request)
                
                # ê²°ê³¼ ê¸°ë¡ (groundedness í‰ê°€ í¬í•¨)
                from app.schemas.drug_analysis import TestRoundWithEvaluation
                test_round = TestRoundWithEvaluation(
                    round_number=round_num,
                    stuff_time=comparison_result.stuff_result.processing_time if comparison_result.stuff_result.success else None,
                    langchain_time=comparison_result.langchain_result.processing_time if comparison_result.langchain_result.success else None,
                    stuff_success=comparison_result.stuff_result.success,
                    langchain_success=comparison_result.langchain_result.success,
                    stuff_evaluation=None,  # Stuff ë°©ì‹ì€ ì„±ëŠ¥ ì¸¡ì •ë§Œ
                    langchain_evaluation=None  # ì´ˆê¸°ê°’
                )
                
                # LangChain ë°©ì‹ì´ ì„±ê³µí•œ ê²½ìš° groundedness í‰ê°€ ì‹¤í–‰ (note í¬í•¨)
                if comparison_result.langchain_result.success:
                    try:
                        # drug_analysis_serviceì˜ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ í•¨ìˆ˜ ì‚¬ìš© (note í¬í•¨)
                        from app.schemas.drug_analysis import DrugInfoAnalysisRequest
                        eval_request = DrugInfoAnalysisRequest(drug_summary=drug_items, note=random_note)
                        context_data = await self.base_service._collect_context_for_evaluation(eval_request)
                        
                        # evaluation_serviceë¥¼ ì§ì ‘ í˜¸ì¶œ (ì´ë¯¸ @traceable ì„¤ì •ë¨, note í¬í•¨)
                        test_round.langchain_evaluation = await evaluation_service.evaluate_drug_analysis(
                            drug_summary=drug_items,
                            analysis_result=comparison_result.langchain_result.result,
                            context_data=context_data,
                            note=random_note
                        )
                    except Exception as eval_error:
                        test_round.langchain_evaluation = None
                test_rounds.append(test_round)
                
                # ê¹”ë”í•œ í•œ ì¤„ ë¡œê·¸ ì¶œë ¥ (note ì •ë³´ í¬í•¨)
                drug_names = [f"{drug['name']}({drug['capacity']})" for drug in random_drugs]
                stuff_time_str = f"{test_round.stuff_time:.0f}ì´ˆ" if test_round.stuff_success else "ì‹¤íŒ¨"
                langchain_time_str = f"{test_round.langchain_time:.0f}ì´ˆ" if test_round.langchain_success else "ì‹¤íŒ¨"
                note_info = f" (í™˜ìë…¸íŠ¸: {random_note[:15]}...)" if random_note else ""
                logger.info(f"[ {round_num:2d} / {request.test_count} ] : {drug_names}{note_info} | Stuff: {stuff_time_str} | Parallel: {langchain_time_str}")
            
            # í†µê³„ ê³„ì‚°
            successful_stuff_times = [r.stuff_time for r in test_rounds if r.stuff_success and r.stuff_time is not None]
            successful_langchain_times = [r.langchain_time for r in test_rounds if r.langchain_success and r.langchain_time is not None]
            
            stuff_success_rate = (sum(1 for r in test_rounds if r.stuff_success) / len(test_rounds)) * 100
            langchain_success_rate = (sum(1 for r in test_rounds if r.langchain_success) / len(test_rounds)) * 100
            
            avg_stuff_time = sum(successful_stuff_times) / len(successful_stuff_times) if successful_stuff_times else None
            avg_langchain_time = sum(successful_langchain_times) / len(successful_langchain_times) if successful_langchain_times else None
            
            summary = {
                "stuff_success_rate": round(stuff_success_rate, 1),
                "langchain_success_rate": round(langchain_success_rate, 1),
                "avg_stuff_time": round(avg_stuff_time, 1) if avg_stuff_time else None,
                "avg_langchain_time": round(avg_langchain_time, 1) if avg_langchain_time else None,
                "stuff_faster_count": sum(1 for r in test_rounds if r.stuff_success and r.langchain_success and r.stuff_time and r.langchain_time and r.stuff_time < r.langchain_time)
            }
            
            total_processing_time = round(time.time() - total_start_time, 2)
            
            # ìµœì¢… ìš”ì•½ ë¡œê·¸ 
            logger.info(f"ğŸ [NíšŒ í‰ê°€ í…ŒìŠ¤íŠ¸] ì™„ë£Œ - ì´ {total_processing_time}ì´ˆ")
            logger.info(f"   ì„±ê³µë¥ : Stuff {summary['stuff_success_rate']}% | Parallel {summary['langchain_success_rate']}%")
            if summary['avg_stuff_time'] and summary['avg_langchain_time']:
                faster = "Stuff" if summary['avg_stuff_time'] < summary['avg_langchain_time'] else "Parallel"
                logger.info(f"   í‰ê· ì‹œê°„: Stuff {summary['avg_stuff_time']}ì´ˆ | Parallel {summary['avg_langchain_time']}ì´ˆ ({faster} ë” ë¹ ë¦„)")
            
            # í‰ê°€ ìš”ì•½ ê³„ì‚°
            evaluation_summary = self._calculate_evaluation_summary(test_rounds)
            
            return PerformanceComparisonNResponse(
                test_rounds=test_rounds,
                test_count=request.test_count,
                drug_count=2,  # í•­ìƒ 2ê°œ ì•½ë¬¼ ì‚¬ìš©
                total_processing_time=total_processing_time,
                performance_summary=summary,
                evaluation_summary=evaluation_summary
            )
            
        except Exception as e:
            logger.error(f"NíšŒ ë°˜ë³µ ì„±ëŠ¥ ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise e
    
    def _calculate_evaluation_summary(self, test_rounds) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ë¼ìš´ë“œ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í‰ê°€ ìš”ì•½ ê³„ì‚°"""
        stats = {}
        
        # Groundedness ì ìˆ˜ ìˆ˜ì§‘
        langchain_groundedness_results = []
        
        for r in test_rounds:
            # StuffëŠ” groundedness í‰ê°€í•˜ì§€ ì•ŠìŒ (ì„±ëŠ¥ ì¸¡ì •ë§Œ)
            if r.langchain_evaluation and r.langchain_evaluation.drug_groundedness_scores:
                for drug_result in r.langchain_evaluation.drug_groundedness_scores:
                    if drug_result.groundedness_score is not None:
                        langchain_groundedness_results.append(drug_result.groundedness_score)
        
        if langchain_groundedness_results:
            avg_langchain_score = sum(langchain_groundedness_results) / len(langchain_groundedness_results)
            stats["per_drug_groundedness"] = {
                "stuff_avg_score": None,  # StuffëŠ” ì„±ëŠ¥ ì¸¡ì •ë§Œ
                "langchain_avg_score": round(avg_langchain_score, 2),
                "stuff_total_evaluations": 0,  # StuffëŠ” ì„±ëŠ¥ ì¸¡ì •ë§Œ
                "langchain_total_evaluations": len(langchain_groundedness_results),
                "hallucination_risk_assessment": {
                    "stuff": "N/A",  # StuffëŠ” ì„±ëŠ¥ ì¸¡ì •ë§Œ
                    "langchain": "Low" if avg_langchain_score >= 0.8 else "Medium" if avg_langchain_score >= 0.6 else "High"
                }
            }
            
            # í‰ê°€ ìš”ì•½ ë¡œê·¸ ì¶œë ¥ (í™˜ê° ë°©ì§€ ì¤‘ì‹¬) - ê¸°ì¡´ ë¡œê·¸ì™€ ì—°ê²°
            logger.info(f"   ê·¼ê±°ì„±: Parallel {avg_langchain_score:.2f} (í™˜ê° ìœ„í—˜: {stats['per_drug_groundedness']['hallucination_risk_assessment']['langchain']}) - StuffëŠ” ì„±ëŠ¥ ì¸¡ì •ë§Œ")
        else:
            stats["per_drug_groundedness"] = None
        
        # JSON validityëŠ” ì œê±° - Claude groundedness í‰ê°€ë§Œ ìˆ˜í–‰
        
        stats["overall_groundedness"] = None  # ì „ì²´ì ì¸ groundednessëŠ” ë³„ë„ë¡œ ê³„ì‚°í•˜ì§€ ì•ŠìŒ
        
        return stats

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
drug_llm_test_service = DrugLLMTestService()