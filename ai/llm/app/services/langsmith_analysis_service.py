"""
LangSmith ë°ì´í„° ë¶„ì„ ì„œë¹„ìŠ¤
"""
import json
import statistics
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, Optional
from langsmith import Client
from app.core.config.config import settings
from app.core.logger import logger
from app.schemas.drug_analysis import (
    MethodStats, EvaluationStats, LangSmithSummaryResponse
)

class LangSmithAnalysisService:
    def __init__(self):
        self.client = None
        if settings.LANGCHAIN_TRACING_V2.lower() == "true" and settings.LANGCHAIN_API_KEY:
            try:
                self.client = Client(api_key=settings.LANGCHAIN_API_KEY)
                logger.info("LangSmith ë¶„ì„ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"LangSmith í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    async def get_langsmith_summary(self) -> LangSmithSummaryResponse:
        """LangSmith í”„ë¡œì íŠ¸ì˜ ì¢…í•© ë¶„ì„ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.client:
            raise Exception("LangSmith í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            project_name = settings.LANGCHAIN_PROJECT
            logger.info(f"ğŸ” LangSmith ë¶„ì„ ëŒ€ìƒ í”„ë¡œì íŠ¸: {project_name}")
            
            # 1. Stuff ë°©ì‹ í†µê³„
            stuff_stats = await self._analyze_method_runs(["abtest", "A"])
            
            # 2. LangChain ë°©ì‹ í†µê³„  
            langchain_stats = await self._analyze_method_runs(["abtest", "B"])
            
            # 3. ì•½ë¬¼ë³„ Groundedness ê·¼ê±°ì„± í‰ê°€ í†µê³„ (í™˜ê° ë°©ì§€ ì¤‘ì‹¬)
            groundedness_stats = await self._analyze_evaluation_runs(["evaluation", "groundedness", "per_drug"])
            
            # 4. ìš”ì•½ ì¸ì‚¬ì´íŠ¸ ìƒì„± (í™˜ê° ë°©ì§€ ì¤‘ì‹¬)
            insights = self._generate_insights(stuff_stats, langchain_stats, groundedness_stats)
            
            # í•œêµ­ì‹œê°„(KST) ê¸°ì¤€ìœ¼ë¡œ ì‹œê°„ í‘œì‹œ
            kst = timezone(timedelta(hours=9))
            current_kst = datetime.now(kst)
            
            return LangSmithSummaryResponse(
                project_name=project_name,
                analysis_period=f"ìµœê·¼ 7ì¼ (ë¶„ì„ ì‹œì : {current_kst.strftime('%Y-%m-%d %H:%M')} KST) - í™˜ê° ë°©ì§€ ì¤‘ì‹¬ ë¶„ì„",
                stuff_method_stats=stuff_stats,
                langchain_method_stats=langchain_stats,
                groundedness_stats=groundedness_stats,
                summary_insights=insights
            )
            
        except Exception as e:
            logger.error(f"LangSmith ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise Exception(f"LangSmith ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

    async def _analyze_method_runs(self, tags: List[str]) -> MethodStats:
        """íŠ¹ì • íƒœê·¸ì˜ ë©”ì„œë“œ ì‹¤í–‰ í†µê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        try:
            # ìµœê·¼ 7ì¼ê°„ ëª¨ë“  ë°ì´í„° ì¡°íšŒ (KST ê¸°ì¤€)
            kst = timezone(timedelta(hours=9))
            current_kst = datetime.now(kst)
            start_time_kst = current_kst - timedelta(days=7)
            start_time_utc = start_time_kst.astimezone(timezone.utc).replace(tzinfo=None)  # LangSmithëŠ” UTC naive í•„ìš”
            
            # ì¼ë°˜ì ì¸ ë°©ì‹ìœ¼ë¡œ ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            all_runs = list(self.client.list_runs(
                project_name=settings.LANGCHAIN_PROJECT,
                start_time=start_time_utc
            ))
            
            # í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œì—ì„œ íƒœê·¸ í•„í„°ë§
            runs = []
            
            for run in all_runs:
                run_tags = getattr(run, 'tags', []) or []
                
                if run_tags:
                    # ëª¨ë“  ìš”êµ¬ íƒœê·¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                    tag_matches = [tag in run_tags for tag in tags]
                    if all(tag_matches):
                        # LLM ì²˜ë¦¬ í•¨ìˆ˜ë§Œ ì¹´ìš´íŠ¸ (ìˆ˜ì •ëœ í•¨ìˆ˜ëª…)
                        if run.name in ['drug_analysis_stuff_llm_processing', 'drug_analysis_langchain_llm_processing']:
                            runs.append(run)
            
            if not runs:
                return MethodStats(
                    total_requests=0,
                    avg_latency=0.0,
                    min_latency=0.0,
                    max_latency=0.0,
                    avg_tokens=None,
                    success_rate=0.0
                )
            
            # ë ˆì´í„´ì‹œ ê³„ì‚° (ë°€ë¦¬ì´ˆ â†’ ì´ˆ ë³€í™˜)
            latencies = []
            token_counts = []
            successful_runs = 0
            
            for run in runs:
                if run.end_time and run.start_time:
                    # ë ˆì´í„´ì‹œ ê³„ì‚° (ì´ˆ ë‹¨ìœ„)
                    latency = (run.end_time - run.start_time).total_seconds()
                    latencies.append(latency)
                    
                    # ì„±ê³µ ì—¬ë¶€ í™•ì¸
                    if not run.error and run.status == "success":
                        successful_runs += 1
            
            # í† í° ì •ë³´ ë³„ë„ ìˆ˜ì§‘ (ChatOpenAI í˜¸ì¶œì—ì„œ)
            token_counts = await self._collect_token_usage(tags, all_runs)
            
            # í†µê³„ ê³„ì‚°
            avg_latency = statistics.mean(latencies) if latencies else 0.0
            min_latency = min(latencies) if latencies else 0.0
            max_latency = max(latencies) if latencies else 0.0
            avg_tokens = statistics.mean(token_counts) if token_counts else None
            success_rate = (successful_runs / len(runs) * 100) if runs else 0.0
            
            return MethodStats(
                total_requests=len(runs),
                avg_latency=round(avg_latency, 3),
                min_latency=round(min_latency, 3),
                max_latency=round(max_latency, 3),
                avg_tokens=round(avg_tokens, 1) if avg_tokens else None,
                success_rate=round(success_rate, 1)
            )
            
        except Exception as e:
            logger.error(f"ë©”ì„œë“œ ë¶„ì„ ì‹¤íŒ¨ (íƒœê·¸: {tags}): {e}")
            raise

    async def _collect_token_usage(self, tags: List[str], all_runs: List) -> List[float]:
        """ChatOpenAI í˜¸ì¶œì—ì„œ í† í° ì‚¬ìš©ëŸ‰ ìˆ˜ì§‘"""
        token_counts = []
        
        for run in all_runs:
            run_tags = getattr(run, 'tags', []) or []
            
            # íƒœê·¸ ë§¤ì¹­í•˜ê³  ChatOpenAIì¸ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
            if run_tags and all(tag in run_tags for tag in tags) and run.name == 'ChatOpenAI':
                # í† í° ì •ë³´ ì¶”ì¶œ
                if hasattr(run, 'outputs') and isinstance(run.outputs, dict):
                    llm_output = run.outputs.get('llm_output', {})
                    if isinstance(llm_output, dict):
                        token_usage = llm_output.get('token_usage', {})
                        if isinstance(token_usage, dict):
                            total_tokens = token_usage.get('total_tokens')
                            if total_tokens:
                                # Stuff ë°©ì‹ì€ ì•½ë¬¼ 2ê°œì”© ì²˜ë¦¬í•˜ë¯€ë¡œ 2ë¡œ ë‚˜ëˆ„ê¸°
                                if "A" in tags:  # Stuff ë°©ì‹
                                    token_counts.append(total_tokens / 2)
                                else:  # LangChain ë°©ì‹ì€ ê°œë³„ ì•½ë¬¼ë³„ ì²˜ë¦¬
                                    token_counts.append(total_tokens)
                
        return token_counts

    async def _analyze_evaluation_runs(self, tags: List[str]) -> EvaluationStats:
        """í‰ê°€ ì‹¤í–‰ í†µê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        try:
            # ìµœê·¼ 7ì¼ê°„ ëª¨ë“  ë°ì´í„° ì¡°íšŒ (KST ê¸°ì¤€)
            kst = timezone(timedelta(hours=9))
            current_kst = datetime.now(kst)
            start_time_kst = current_kst - timedelta(days=7)
            start_time_utc = start_time_kst.astimezone(timezone.utc).replace(tzinfo=None)
            
            # ì¼ë°˜ì ì¸ ë°©ì‹ìœ¼ë¡œ ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            all_runs = list(self.client.list_runs(
                project_name=settings.LANGCHAIN_PROJECT,
                start_time=start_time_utc
            ))
            
            # í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œì—ì„œ íƒœê·¸ í•„í„°ë§
            runs = []
            
            for run in all_runs:
                run_tags = getattr(run, 'tags', []) or []
                
                if run_tags:
                    # ëª¨ë“  ìš”êµ¬ íƒœê·¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                    tag_matches = [tag in run_tags for tag in tags]
                    if all(tag_matches):
                        # single_drug_groundedness_evaluation í•¨ìˆ˜ë§Œ ì¹´ìš´íŠ¸ (ê°œë³„ ì•½ë¬¼ í‰ê°€ë§Œ)
                        if run.name == 'single_drug_groundedness_evaluation':
                            runs.append(run)
            
            if not runs:
                return EvaluationStats(
                    total_evaluations=0,
                    avg_score=0.0,
                    min_score=0.0,
                    max_score=0.0,
                    std_deviation=0.0
                )
            
            # í‰ê°€ ì ìˆ˜ ì¶”ì¶œ
            scores = []
            
            for run in runs:
                if run.outputs:
                    try:
                        # outputsì—ì„œ ì ìˆ˜ ì¶”ì¶œ (ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€)
                        logger.debug(f"Run {run.name} outputs êµ¬ì¡°: {type(run.outputs)} - {run.outputs}")
                        
                        if isinstance(run.outputs, dict):
                            score = run.outputs.get('score')
                            if isinstance(score, (int, float)):
                                scores.append(float(score))
                                # 0.0ì¸ ê²½ìš° ì „ì²´ ì •ë³´ ì¶œë ¥
                                if float(score) == 0.0:
                                    logger.error(f"â˜… 0ì  ë°œê²¬! Run ID: {getattr(run, 'id', 'N/A')}")
                                    logger.error(f"â˜… 0ì  Run ì´ë¦„: {run.name}")
                                    logger.error(f"â˜… 0ì  Run ì „ì²´ outputs: {run.outputs}")
                                    logger.error(f"â˜… 0ì  Run íƒœê·¸: {getattr(run, 'tags', [])}")
                                    logger.error(f"â˜… 0ì  Run ì‹œì‘ì‹œê°„: {getattr(run, 'start_time', 'N/A')}")
                                    logger.error(f"â˜… 0ì  Run ì¢…ë£Œì‹œê°„: {getattr(run, 'end_time', 'N/A')}")
                                    logger.error(f"â˜… 0ì  Run ì—ëŸ¬: {getattr(run, 'error', 'N/A')}")
                                    logger.error(f"â˜… 0ì  Run ìƒíƒœ: {getattr(run, 'status', 'N/A')}")
                                else:
                                    logger.debug(f"ì •ìƒ ì ìˆ˜ ì¶”ê°€ë¨: {score}")
                            else:
                                # 'score' í‚¤ê°€ ì—†ê±°ë‚˜ ê°’ì´ ì˜¬ë°”ë¥´ì§€ ì•Šì€ ê²½ìš° ì „ì²´ êµ¬ì¡° í™•ì¸
                                logger.debug(f"ì ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - outputs í‚¤ë“¤: {list(run.outputs.keys())}")
                        elif isinstance(run.outputs, str):
                            # JSON ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹± ì‹œë„
                            try:
                                output_data = json.loads(run.outputs)
                                if isinstance(output_data, dict) and 'score' in output_data:
                                    score = float(output_data['score'])
                                    scores.append(score)
                                    # 0.0ì¸ ê²½ìš° ì „ì²´ ì •ë³´ ì¶œë ¥
                                    if score == 0.0:
                                        logger.error(f"â˜… JSONì—ì„œ 0ì  ë°œê²¬! Run ID: {getattr(run, 'id', 'N/A')}")
                                        logger.error(f"â˜… JSON 0ì  Run ì´ë¦„: {run.name}")
                                        logger.error(f"â˜… JSON 0ì  ì „ì²´ outputs: {run.outputs}")
                                        logger.error(f"â˜… JSON 0ì  íŒŒì‹±ëœ ë°ì´í„°: {output_data}")
                                        logger.error(f"â˜… JSON 0ì  Run ì—ëŸ¬: {getattr(run, 'error', 'N/A')}")
                                    else:
                                        logger.debug(f"JSONì—ì„œ ì •ìƒ ì ìˆ˜ ì¶”ê°€ë¨: {score}")
                            except json.JSONDecodeError:
                                logger.debug(f"JSON íŒŒì‹± ì‹¤íŒ¨: {run.outputs}")
                                continue
                    except Exception as e:
                        logger.debug(f"ì ìˆ˜ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
            
            # í†µê³„ ê³„ì‚°
            if not scores:
                logger.warning(f"ì ìˆ˜ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŒ. ì´ run ê°œìˆ˜: {len(runs)}")
                return EvaluationStats(
                    total_evaluations=len(runs),
                    avg_score=0.0,
                    min_score=0.0,
                    max_score=0.0,
                    std_deviation=0.0
                )
            
            avg_score = statistics.mean(scores)
            min_score = min(scores)
            max_score = max(scores)
            std_dev = statistics.stdev(scores) if len(scores) > 1 else 0.0
            
            logger.info(f"ê³„ì‚°ëœ í†µê³„: avg={avg_score:.2f}, min={min_score:.2f}, max={max_score:.2f}, std={std_dev:.2f}")
            
            return EvaluationStats(
                total_evaluations=len(runs),
                avg_score=round(avg_score, 2),
                min_score=round(min_score, 2),
                max_score=round(max_score, 2),
                std_deviation=round(std_dev, 2)
            )
            
        except Exception as e:
            logger.error(f"í‰ê°€ ë¶„ì„ ì‹¤íŒ¨ (íƒœê·¸: {tags}): {e}")
            raise

    def _generate_insights(self, stuff_stats: MethodStats, langchain_stats: MethodStats, groundedness_stats: EvaluationStats) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™˜ê° ë°©ì§€ ì¤‘ì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        insights = {}
        
        # ì„±ëŠ¥ ë¹„êµ
        if stuff_stats.avg_latency > 0 and langchain_stats.avg_latency > 0:
            if stuff_stats.avg_latency < langchain_stats.avg_latency:
                faster_method = "Stuff"
                improvement = ((langchain_stats.avg_latency - stuff_stats.avg_latency) / langchain_stats.avg_latency) * 100
            else:
                faster_method = "LangChain"
                improvement = ((stuff_stats.avg_latency - langchain_stats.avg_latency) / stuff_stats.avg_latency) * 100
            
            insights["performance"] = {
                "faster_method": faster_method,
                "improvement_percentage": round(improvement, 1),
                "latency_difference": round(abs(stuff_stats.avg_latency - langchain_stats.avg_latency), 3)
            }
        
        # ì•ˆì •ì„± ë¹„êµ
        insights["reliability"] = {
            "more_stable_method": "Stuff" if stuff_stats.success_rate >= langchain_stats.success_rate else "LangChain",
            "stuff_success_rate": stuff_stats.success_rate,
            "langchain_success_rate": langchain_stats.success_rate
        }
        
        # í† í° íš¨ìœ¨ì„±
        if stuff_stats.avg_tokens and langchain_stats.avg_tokens:
            insights["token_efficiency"] = {
                "more_efficient_method": "Stuff" if stuff_stats.avg_tokens <= langchain_stats.avg_tokens else "LangChain",
                "stuff_avg_tokens": stuff_stats.avg_tokens,
                "langchain_avg_tokens": langchain_stats.avg_tokens
            }
        
        # ì•½ë¬¼ë³„ ê·¼ê±°ì„± í‰ê°€ (í™˜ê° ë°©ì§€ ì¤‘ì‹¬)
        if groundedness_stats.total_evaluations > 0:
            grounding_level = "ë†’ìŒ" if groundedness_stats.avg_score >= 0.8 else "ë³´í†µ" if groundedness_stats.avg_score >= 0.6 else "ë‚®ìŒ"
            grounding_consistency = "ì¼ê´€ì " if groundedness_stats.std_deviation <= 0.1 else "ë³´í†µ" if groundedness_stats.std_deviation <= 0.2 else "ë¶ˆì•ˆì •"
            
            insights["hallucination_prevention"] = {
                "grounding_level": grounding_level,
                "consistency_level": grounding_consistency,
                "avg_grounding_score": groundedness_stats.avg_score,
                "grounding_variance": groundedness_stats.std_deviation,
                "hallucination_risk": "ë‚®ìŒ" if groundedness_stats.avg_score >= 0.8 else "ë³´í†µ" if groundedness_stats.avg_score >= 0.6 else "ë†’ìŒ",
                "per_drug_evaluation": True,
                "focus": "RAG í™˜ê²½ í™˜ê° ë°©ì§€"
            }
        
        return insights

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
langsmith_analysis_service = LangSmithAnalysisService()